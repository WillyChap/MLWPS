{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745474da-c203-4382-8575-65ed6f2fad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import time\n",
    "from dask import delayed\n",
    "from dask import delayed, persist\n",
    "import dask\n",
    "import xarray as xr\n",
    "from dask import delayed\n",
    "from dask.diagnostics import ProgressBar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09fe031-4e84-49bf-a16b-541491b92ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client does not exist yet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/apps/opt/conda/envs/npl-2023b/lib/python3.10/site-packages/dask_jobqueue/pbs.py:82: FutureWarning: project has been renamed to account as this kwarg was used wit -A option. You are still using it (please also check config files). If you did not set account yet, project will be respected for now, but it will be removed in a future release. If you already set account, project is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n",
      "/glade/u/apps/opt/conda/envs/npl-2023b/lib/python3.10/site-packages/dask_jobqueue/pbs.py:82: FutureWarning: project has been renamed to account as this kwarg was used wit -A option. You are still using it (please also check config files). If you did not set account yet, project will be respected for now, but it will be removed in a future release. If you already set account, project is ignored and you can remove it.\n",
      "  warnings.warn(warn, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-e889115b-8244-11ee-a682-3cecef1b123c</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/wchapman/Dingo/proxy/8787/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/wchapman/Dingo/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"https://jupyterhub.hpc.ucar.edu/stable/user/wchapman/Dingo/proxy/8787/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">f2721b92</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/wchapman/Dingo/proxy/8787/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/wchapman/Dingo/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-32d5ffba-9689-436c-84f1-c33baad72813</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://128.117.208.67:45655\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/wchapman/Dingo/proxy/8787/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/wchapman/Dingo/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://128.117.208.67:45655' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### settings !!! MODIFY THIS BLOCK\n",
    "start_date = '2012-01-01'\n",
    "end_date = '2012-12-31' #make sure this date is after the start date... \n",
    "interval_hours = 1 #what hour interval would you like to get? [i.e: 1 = 24 files/day, 6 = 4 files/day]\n",
    "FPout = '/glade/derecho/scratch/wchapman/ERA5_regrid_out/' #where do you want the files stored?\n",
    "prefix_out = 'ERA5_e5.oper.ml.v3' #what prefix do you want the files stored with?\n",
    "project_num = 'NAML0001'\n",
    "#### settings !!! MODIFY THIS BLOCK\n",
    "\n",
    "if 'client' in locals():\n",
    "    client.shutdown()\n",
    "    print('...shutdown client...')\n",
    "else:\n",
    "    print('client does not exist yet')\n",
    "    \n",
    "###dask NCAR client: \n",
    "from distributed import Client\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "cluster = PBSCluster(project='NAML0001',walltime='12:00:00',cores=1, memory='25GB',shared_temp_directory='/glade/scratch/wchapman/tmp',queue='casper')\n",
    "cluster.scale(jobs=40)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70bccb9-da8c-40af-b18b-5b7d3571f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_strings_with_substring(string_list, substring):\n",
    "    # Initialize an empty list to store matching strings\n",
    "    matching_strings = []\n",
    "\n",
    "    # Iterate through the list\n",
    "    for string in string_list:\n",
    "        # Check if the specified substring is present in the current string\n",
    "        if substring in string:\n",
    "            matching_strings.append(string)\n",
    "\n",
    "    # Return the list of matching strings\n",
    "    return matching_strings\n",
    "\n",
    "def flatten_list(input_list):\n",
    "    flattened_list = []\n",
    "    for item in input_list:\n",
    "        if isinstance(item, list):\n",
    "            flattened_list.extend(flatten_list(item))\n",
    "        else:\n",
    "            flattened_list.append(item)\n",
    "    return flattened_list\n",
    "\n",
    "##function get file paths ... \n",
    "def fp_dates_wanted(Dateswanted):\n",
    "    years_wanted = Dateswanted[:].year\n",
    "    months_wanted = Dateswanted[:].month\n",
    "    day_wanted = Dateswanted[:].day\n",
    "    \n",
    "    list_yrm =[]\n",
    "    for ywmw in zip(years_wanted,months_wanted):\n",
    "        list_yrm.append(str(ywmw[0])+f'{ywmw[1]:02}')\n",
    "    \n",
    "    fp_t = []\n",
    "    fp_u = []\n",
    "    fp_v = []\n",
    "    fp_q = []\n",
    "    fp_ps = []\n",
    "    \n",
    "    lastday = str(Dateswanted[-1])[:10]\n",
    "    \n",
    "    for yrm_fp in np.unique(list_yrm):\n",
    "        for dayday in np.unique(day_wanted):\n",
    "            \n",
    "            \n",
    "            fp_u.append(sorted(glob.glob('/glade/campaign/collections/rda/data/ds633.6/e5.oper.an.ml/'+yrm_fp+'/'+'*_u*'+yrm_fp+f'{dayday:02}'+'*.nc')))\n",
    "            fp_v.append(sorted(glob.glob('/glade/campaign/collections/rda/data/ds633.6/e5.oper.an.ml/'+yrm_fp+'/'+'*_v*'+yrm_fp+f'{dayday:02}'+'*.nc')))\n",
    "            fp_t.append(sorted(glob.glob('/glade/campaign/collections/rda/data/ds633.6/e5.oper.an.ml/'+yrm_fp+'/'+'*_t*'+yrm_fp+f'{dayday:02}'+'*.nc')))\n",
    "            fp_q.append(sorted(glob.glob('/glade/campaign/collections/rda/data/ds633.6/e5.oper.an.ml/'+yrm_fp+'/'+'*_q*'+yrm_fp+f'{dayday:02}'+'*.nc')))\n",
    "            fp_ps.append(sorted(glob.glob('/glade/campaign/collections/rda/data/ds633.6/e5.oper.an.ml/'+yrm_fp+'/'+'*_sp*'+yrm_fp+f'{dayday:02}'+'*.nc')))\n",
    "            \n",
    "            if yrm_fp[:4]+'-'+yrm_fp[4:]+'-'+f'{dayday:02}' == lastday:\n",
    "                break\n",
    "\n",
    "    fp_u = flatten_list(fp_u)\n",
    "    fp_v = flatten_list(fp_v)\n",
    "    fp_t = flatten_list(fp_t)\n",
    "    fp_q = flatten_list(fp_q)\n",
    "    fp_ps = flatten_list(fp_ps)\n",
    "    \n",
    "    files_dict ={'u':np.unique(fp_u),'v':np.unique(fp_v),'t':np.unique(fp_t),'q':np.unique(fp_q),'ps':np.unique(fp_ps)}\n",
    "    \n",
    "    \n",
    "    return files_dict \n",
    "\n",
    "def make_nc_files(files_dict,Dateswanted,Dayswanted):    \n",
    "    for dw in Dayswanted:\n",
    "        print(str(dw)[:10])\n",
    "        substring_match = str(dw)[:4]+str(dw)[5:7]+str(dw)[8:10]\n",
    "        smatch_u = find_strings_with_substring(files_dict['u'], substring_match)\n",
    "        smatch_v = find_strings_with_substring(files_dict['v'], substring_match)\n",
    "        smatch_t = find_strings_with_substring(files_dict['t'], substring_match)\n",
    "        smatch_q = find_strings_with_substring(files_dict['q'], substring_match)\n",
    "        smatch_ps = find_strings_with_substring(files_dict['ps'], substring_match)\n",
    "        DS_u= xr.open_mfdataset(smatch_u)\n",
    "        sel_times = Dateswanted.intersection(DS_u['time'])\n",
    "        DS_v= xr.open_mfdataset(smatch_v).sel(time=sel_times)\n",
    "        DS_t= xr.open_mfdataset(smatch_t).sel(time=sel_times)\n",
    "        DS_q= xr.open_mfdataset(smatch_q).sel(time=sel_times)\n",
    "        DS_ps= xr.open_mfdataset(smatch_ps).sel(time=sel_times)\n",
    "        print('loading')\n",
    "        DS=xr.merge([DS_u.sel(time=sel_times),DS_v,DS_t,DS_q]).load()\n",
    "        print('loaded')\n",
    "        \n",
    "        for ee,tt in enumerate(DS['time']):\n",
    "            hourdo = DS['time.hour'][ee]\n",
    "            \n",
    "            datstr = str(dw)[:4]+str(dw)[5:7]+str(dw)[8:10]+f'{hourdo:02}'\n",
    "            #DS.sel(time=tt).squeeze().to_netcdf()\n",
    "            out_file=+'/' +prefix_out +'.uvtq.'+ datstr+'.nc'\n",
    "            write_job = DS.sel(time=tt).squeeze().to_netcdf(out_file,compute=False)\n",
    "            with ProgressBar():\n",
    "                print(f\"Writing to {out_file}\")\n",
    "                write_job.compute()      \n",
    "            print(out_file) \n",
    "            out_file=FPout+'/' +prefix_out +'.ps.'+ datstr+'.nc'\n",
    "            DS_ps['Z_GDS4_SFC'] = xr.zeros_like(DS_ps['SP'])\n",
    "            DS_ps['Z_GDS4_SFC'][:,:]=Static_zheight['Z_GDS4_SFC'].values\n",
    "            write_job = DS_ps.sel(time=tt).squeeze().to_netcdf(out_file,compute=False)\n",
    "            with ProgressBar():\n",
    "                print(f\"Writing to {out_file}\")\n",
    "                write_job.compute()    \n",
    "            print(out_file) \n",
    "\n",
    "    return DS,DS_ps\n",
    "\n",
    "\n",
    "def make_nc_files_optimized(files_dict, Dateswanted, Dayswanted, FPout, prefix_out):\n",
    "    \"\"\"\n",
    "    Optimized function to perform a specific task using Dask with specified resources.\n",
    "\n",
    "    Parameters:\n",
    "    - files_dict: A dictionary of files.\n",
    "    - Dateswanted: List of dates.\n",
    "    - Dayswanted: List of days.\n",
    "    - FPout: Output file path.\n",
    "    - prefix_out: Output file prefix.\n",
    "\n",
    "    Returns:\n",
    "    - delayed_writes: List of delayed write operations.\n",
    "    \"\"\"\n",
    "    Static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "    \n",
    "    delayed_writes = []\n",
    "    for dw in Dayswanted:\n",
    "        print(str(dw)[:10])\n",
    "        substring_match = str(dw)[:4] + str(dw)[5:7] + str(dw)[8:10]\n",
    "        smatch_u = find_strings_with_substring(files_dict['u'], substring_match)\n",
    "        smatch_v = find_strings_with_substring(files_dict['v'], substring_match)\n",
    "        smatch_t = find_strings_with_substring(files_dict['t'], substring_match)\n",
    "        smatch_q = find_strings_with_substring(files_dict['q'], substring_match)\n",
    "        smatch_ps = find_strings_with_substring(files_dict['ps'], substring_match)\n",
    "        \n",
    "        DS_u = xr.open_mfdataset(smatch_u, parallel=True)\n",
    "        sel_times = Dateswanted.intersection(DS_u['time'])\n",
    "        DS_v = xr.open_mfdataset(smatch_v, parallel=True).sel(time=sel_times)\n",
    "        DS_t = xr.open_mfdataset(smatch_t, parallel=True).sel(time=sel_times)\n",
    "        DS_q = xr.open_mfdataset(smatch_q, parallel=True).sel(time=sel_times)\n",
    "        DS_ps = xr.open_mfdataset(smatch_ps, parallel=True).sel(time=sel_times)\n",
    "        \n",
    "        print('loading')\n",
    "        DS = xr.merge([DS_u.sel(time=sel_times), DS_v, DS_t, DS_q])\n",
    "        print('copying variables')\n",
    "        DS['US'] = DS['U'].copy(deep=True)\n",
    "        DS['VS'] = DS['V'].copy(deep=True)\n",
    "        print('loaded')\n",
    "        \n",
    "        for ee, tt in enumerate(DS['time']):\n",
    "            hourdo = DS['time.hour'][ee]\n",
    "            datstr = str(dw)[:4] + str(dw)[5:7] + str(dw)[8:10] + f'{hourdo:02}'\n",
    "            \n",
    "            out_file_uvtq = FPout + '/' + prefix_out + '.uvtq.' + datstr + '.nc'\n",
    "            delayed_write_uvtq = delayed(DS.sel(time=tt).squeeze().to_netcdf)(out_file_uvtq)\n",
    "            delayed_writes.append(delayed_write_uvtq)\n",
    "            \n",
    "            out_file_ps = FPout + '/' + prefix_out + '.ps.' + datstr + '.nc'\n",
    "            DS_ps['Z_GDS4_SFC'] = xr.zeros_like(DS_ps['SP'])\n",
    "            DS_ps['Z_GDS4_SFC'][:, :] = Static_zheight['Z_GDS4_SFC'].values\n",
    "            delayed_write_ps = delayed(DS_ps.sel(time=tt).squeeze().to_netcdf)(out_file_ps)\n",
    "            delayed_writes.append(delayed_write_ps)\n",
    "\n",
    "    # Compute the delayed write operations concurrently\n",
    "    with ProgressBar():\n",
    "        delayed_writes = list(dask.compute(*delayed_writes))\n",
    "\n",
    "    return delayed_writes\n",
    "\n",
    "\n",
    "\n",
    "def divide_datetime_index(date_index, max_items_per_division=4):\n",
    "    \"\"\"\n",
    "    Divide a DatetimeIndex into sublists with a maximum number of items per division.\n",
    "\n",
    "    Parameters:\n",
    "    - date_index: DatetimeIndex to be divided.\n",
    "    - max_items_per_division: Maximum number of items per division (default is 4).\n",
    "\n",
    "    Returns:\n",
    "    - divided_lists: List of sublists.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the divided lists\n",
    "    divided_lists = []\n",
    "\n",
    "    # Initialize a sublist with the first date\n",
    "    sublist = [date_index[0]]\n",
    "\n",
    "    # Iterate through the remaining dates\n",
    "    for date in date_index[1:]:\n",
    "        # Add the current date to the sublist\n",
    "        sublist.append(date)\n",
    "\n",
    "        # Check if the sublist has reached the maximum allowed size\n",
    "        if len(sublist) == max_items_per_division:\n",
    "            # If it has, add the sublist to the divided_lists and reset the sublist\n",
    "            divided_lists.append(sublist)\n",
    "            sublist = []\n",
    "\n",
    "    # If there are remaining items in the sublist, add it to the divided_lists\n",
    "    if sublist:\n",
    "        divided_lists.append(sublist)\n",
    "\n",
    "    # Ensure that every division has at least two items by merging the last two divisions if necessary\n",
    "    if len(divided_lists[-1]) < 2 and len(divided_lists) > 1:\n",
    "        last_two_lists = divided_lists[-2:]  # Get the last two divisions\n",
    "        combined_list = sum(last_two_lists, [])  # Combine them\n",
    "        divided_lists = divided_lists[:-2]  # Remove the last two divisions\n",
    "        divided_lists.append(combined_list)  # Add the combined list back\n",
    "\n",
    "    return divided_lists\n",
    "\n",
    "def increment_date_by_one_day(date_str):\n",
    "    \"\"\"\n",
    "    Increment a date by one day and return it as a string.\n",
    "\n",
    "    Parameters:\n",
    "    - date_str: Input date string in the format 'YYYY-MM-DD'.\n",
    "\n",
    "    Returns:\n",
    "    - incremented_date_str: Date string incremented by one day.\n",
    "    \"\"\"\n",
    "    # Convert the input date string to a pandas Timestamp\n",
    "    date = pd.Timestamp(date_str)\n",
    "\n",
    "    # Increment the date by one day\n",
    "    incremented_date = date + pd.DateOffset(days=1)\n",
    "\n",
    "    # Convert the incremented date back to a string in the same format\n",
    "    incremented_date_str = incremented_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    return incremented_date_str\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     #look at all the dates:\n",
    "#     Dayswantedtot = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'D')\n",
    "#     #look at all the dates:\n",
    "#     print(len(Dayswantedtot))\n",
    "#     if len(Dayswantedtot)<4:\n",
    "#         start_time = time.time()  # Record the start time\n",
    "#         Dayswanted = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'D')\n",
    "#         Dateswanted = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'H')\n",
    "#         Static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "#         files_dict=fp_dates_wanted(Dateswanted)\n",
    "#         #make the files:\n",
    "#         print('...starting processing...')\n",
    "#         delayed_writes = make_nc_files_optimized(files_dict, Dateswanted, Dayswanted,FPout, prefix_out)\n",
    "#         elapsed_time = time.time() - start_time\n",
    "#         print(f\" executed in {elapsed_time} seconds\")\n",
    "#     else: \n",
    "#         print('in here!!')\n",
    "#         divided_lists =divide_datetime_index(Dayswantedtot)\n",
    "\n",
    "#         for dd in divided_lists:\n",
    "#             strtd = str(dd[0])[:10]\n",
    "#             endd  = str(dd[-1])[:10]\n",
    "#             endd  = increment_date_by_one_day(endd)\n",
    "#             print('doing files:',strtd,endd)\n",
    "#             start_time = time.time()  # Record the start time\n",
    "#             Dayswanted = pd.date_range(start=strtd,end=endd,freq=str(interval_hours)+'D')\n",
    "#             Dateswanted = pd.date_range(start=strtd,end=endd,freq=str(interval_hours)+'H')\n",
    "#             Static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "#             files_dict=fp_dates_wanted(Dateswanted)\n",
    "#             #make the files:\n",
    "#             print('...starting processing...')\n",
    "#             delayed_writes = make_nc_files_optimized(files_dict, Dateswanted, Dayswanted,FPout, prefix_out)\n",
    "#             elapsed_time = time.time() - start_time\n",
    "#             print(f\" phase executed in {elapsed_time} seconds\")\n",
    "            \n",
    "    \n",
    "#     if 'client' in locals():\n",
    "#         client.shutdown()\n",
    "#         print('...shutdown client...')\n",
    "#     else:\n",
    "#         print('client does not exist yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc37b52a-c869-4df7-932d-0d18b04bc6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding files\n"
     ]
    }
   ],
   "source": [
    "start_date = '2012-01-01'\n",
    "end_date = '2013-01-01' #make sure this date is after the start date... \n",
    "interval_hours = 1 #what hour interval would you like to get? [i.e: 1 = 24 files/day, 6 = 4 files/day]\n",
    "Dayswantedtot = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'D')\n",
    "Dayswanted = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'D')\n",
    "Dateswanted = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'H')\n",
    "Static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "#fp_dates_wanted\n",
    "print('finding files')\n",
    "files_dict=fp_dates_wanted(Dateswanted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461dc75-133f-40bf-bef3-da587810a087",
   "metadata": {},
   "source": [
    "## Hit It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a6163380-2fb9-4d78-b18b-764907cc8455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u\n"
     ]
    }
   ],
   "source": [
    "start_date = '2012-01-01'\n",
    "end_date = '2013-01-01' #make sure this date is after the start date... \n",
    "params = {'start_date':start_date,'end_date':end_date,'interval_hours':interval_hours,'levs_train':[10,20,30,40,50,60,70,80,90,100,110,120,130]}\n",
    "trainset = INC_Dataset_mf(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52ed82ee-fd1b-47e7-aa5f-2f043ba5b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset\n",
    "import random\n",
    "# print(\"PyTorch Version: \", torch.__version__)\n",
    "def load_files(fnms,**kwargs):\n",
    "    lvs=kwargs['levs']\n",
    "    DSm = xr.open_mfdataset(fnms,drop_variables=['weight','utc_date','a_half','zero','a_model','b_model','b_half'],parallel=True).sel(level=lvs)\n",
    "    return DSm\n",
    "\n",
    "# class INC_Dataset_mf(Dataset):\n",
    "\n",
    "class INC_Dataset_mf():\n",
    "    def __init__(self,params,shuff=True):\n",
    "        self.params = params\n",
    "        self.start_date = params['start_date']\n",
    "        self.end_date = params['end_date']\n",
    "        self.levels = params['levs_train']\n",
    "        self.dayswanted = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'D')\n",
    "        self.dateswanted = pd.date_range(start=start_date,end=end_date,freq=str(interval_hours)+'H')\n",
    "        self.static_zheight = xr.open_dataset('/glade/u/home/wchapman/RegriddERA5_CAMFV/static_operation_ERA5_zhght.nc')\n",
    "        self.static_topo = np.expand_dims(np.array(self.static_zheight['Z_GDS4_SFC']),axis=[0])\n",
    "        self.files_dict = fp_dates_wanted(self.dateswanted)\n",
    "        first_key = next(iter(self.files_dict))\n",
    "        self.num_files = len(self.files_dict[first_key])\n",
    "                        \n",
    "        self.steps_per_file= len(xr.open_dataset(self.files_dict[first_key][0])['time'])\n",
    "        self.total_ts = (self.num_files*self.steps_per_file)-self.steps_per_file # this is done so the last file can utilize 2 steps \n",
    "        self.file_tracker = list(np.arange(self.num_files-1)) # this is done so the last file can utilize 2 steps \n",
    "        self.samples_todo = np.arange(self.steps_per_file) \n",
    "        \n",
    "        self.shuff =shuff\n",
    "        if self.shuff:\n",
    "            olist = list(np.arange(self.num_files-1))\n",
    "            self.file_tracker=list(random.sample(olist, len(olist)))\n",
    "            self.samples_todo = list(np.arange(self.steps_per_file))\n",
    "        else:\n",
    "            self.file_tracker = list(np.arange(self.num_files-1))\n",
    "            self.samples_todo = list(np.arange(self.steps_per_file))\n",
    "           \n",
    "        self.fchoice=0\n",
    "        self.load_new(self.fchoice,self.params)\n",
    "        self.count_up=0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(self.total_ts)\n",
    "    \n",
    "    def load_new(self, file_index, params):\n",
    "\n",
    "        self.DSm = self.preprocess(file_index, self.params)\n",
    "        self.x_ = np.array(dedo.DSm.load().to_array())\n",
    "        \n",
    "    def preprocess(self,train_data,params):\n",
    "        \n",
    "        keys=list(dedo.files_dict)\n",
    "        \n",
    "        keys_files = list(dedo.files_dict)\n",
    "        fls = []\n",
    "        for kf in keys_files:\n",
    "            fls.append(dedo.files_dict[kf][file_index])\n",
    "            fls.append(dedo.files_dict[kf][file_index+1])\n",
    "        \n",
    "        \n",
    "        load_data_kwargs={'levs':self.levels}\n",
    "        \n",
    "        train_ = load_files(fls,**load_data_kwargs)\n",
    "        \n",
    "        return train_\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx = self.get_indices(idx)\n",
    "                \n",
    "        # Update sample index for the current file\n",
    "        # If all samples in the current file are processed, move to the next file\n",
    "        if self.count_up >= len(.samples_todo):\n",
    "            self.fchoice+=1\n",
    "            \n",
    "            try: \n",
    "                self.load_new(self.fchoice, self.params)\n",
    "            except: \n",
    "                self.load_new(0, self.params)\n",
    "            \n",
    "            \n",
    "            if self.shuff:\n",
    "                self.samples_todo = list(np.arange(self.steps_per_file))             \n",
    "            else: \n",
    "                self.samples_todo = list(np.arange(self.steps_per_file))\n",
    "            self.count_up = 0\n",
    "            \n",
    "        x_pre = self.x_[:,sample_idx,:,:,:]\n",
    "        x_pre_shp = x_pre.shape\n",
    "        x = np.reshape(x_pre,[x_pre_shp[0]*x_pre_shp[1],x_pre_shp[2],x_pre_shp[3]])\n",
    "        x = np.concatenate([x,dedo.static_topo],axis=0)\n",
    "        \n",
    "        y1_pre = self.x_[:,sample_idx+1,:,:,:]\n",
    "        y1 = np.reshape(y1_pre,[x_pre_shp[0]*x_pre_shp[1],x_pre_shp[2],x_pre_shp[3]])\n",
    "        \n",
    "        y2_pre = self.x_[:,sample_idx+2,:,:,:]\n",
    "        y2 = np.reshape(y2_pre,[x_pre_shp[0]*x_pre_shp[1],x_pre_shp[2],x_pre_shp[3]])\n",
    "        self.count_up += 1\n",
    "        \n",
    "        x = torch.tensor(x).float()\n",
    "        y1 = torch.tensor(y1).float()\n",
    "        y2 = torch.tensor(y2).float()\n",
    "        y  = (y1,y2)\n",
    "        \n",
    "        return x,y\n",
    "    \n",
    "    def get_indices(self, global_idx):\n",
    "        sample_idx = self.samples_todo[self.count_up]\n",
    "        return sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "efd8c581-9fa1-47f8-ab85-4bcf4afe70b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/glade/scratch/wchapman/DA_ML/CESML_AI/Data/daychunks/f.e21.FHIST.f09_f09_mg17_nudge_it_ERA5_19990101_19990110.val.000.nc'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fns[0]\n",
    "fns[0].replace(\"train\", \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "65e5e6d7-6859-4da8-9174-5c39509a5047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/glade/scratch/wchapman/DA_ML/CESML_AI/Data/daychunks/f.e21.FHIST.f09_f09_mg17_nudge_it_ERA5_19990421_19990430.test.011.nc\n",
      "/glade/scratch/wchapman/DA_ML/CESML_AI/Data/daychunks/f.e21.FHIST.f09_f09_mg17_nudge_it_ERA5_19990501_19990510.test.012.nc\n",
      "/glade/scratch/wchapman/DA_ML/CESML_AI/Data/daychunks/f.e21.FHIST.f09_f09_mg17_nudge_it_ERA5_19990511_19990520.test.013.nc\n"
     ]
    }
   ],
   "source": [
    "import shutil \n",
    "for ee,ff in enumerate(fns[11:14]): \n",
    "    new_name = ff.replace(\"train\", \"test\")\n",
    "    print(new_name)\n",
    "    shutil.copy2(ff, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71cbdcb-d2ad-4947-b999-e99044e41020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2023b",
   "language": "python",
   "name": "npl-2023b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
