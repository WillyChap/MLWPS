{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e125b2a2-f8ad-4d23-a3e2-6991117f7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a 3D version of https://github.com/lucidrains/DALLE2-pytorch/blob/680dfc4d93b70f9ab23c814a22ca18017a738ef6/dalle2_pytorch/vqgan_vae.py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from math import sqrt\n",
    "from vector_quantize_pytorch import LFQ\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad as torch_grad\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64ea216-050b-4199-86c4-bce23099a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd70e94c-166f-4ea8-a32b-fcbd69937b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncDec(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_height, \n",
    "        patch_height,\n",
    "        image_width,\n",
    "        patch_width,\n",
    "        frames, \n",
    "        frame_patch_size,\n",
    "        dim,\n",
    "        channels = 3,\n",
    "        depth = 4,\n",
    "        heads = 8,\n",
    "        dim_head = 32,\n",
    "        mlp_dim = 32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width) * (frames // frame_patch_size)\n",
    "        input_dim = channels * patch_height * patch_width * frame_patch_size\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            Rearrange('b c (f pf) (h p1) (w p2) -> b (f h w) (p1 p2 pf c)', p1 = patch_height, p2 = patch_width, pf = frame_patch_size),\n",
    "            nn.Linear(input_dim, dim),\n",
    "            Transformer(\n",
    "                dim = dim,\n",
    "                depth = depth,\n",
    "                dim_head = dim_head,\n",
    "                heads = heads,\n",
    "                mlp_dim = mlp_dim\n",
    "            ),\n",
    "             #Rearrange('b (f h w) c -> b c f h w', h = patch_height, w = patch_width)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            #Rearrange('b c f h w -> b (f h w) c'),\n",
    "            Transformer(\n",
    "                dim = dim,\n",
    "                depth = depth,\n",
    "                dim_head = dim_head,\n",
    "                heads = heads,\n",
    "                mlp_dim = mlp_dim\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, dim * 4, bias = False),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(dim * 4, input_dim, bias = False),\n",
    "            ),\n",
    "            Rearrange('b (f h w) (p1 p2 pf c) -> b c (pf f) (p1 h) (w p2)', \n",
    "                      h = (image_height // patch_height), f = (frames // frame_patch_size), p1 = patch_height, p2 = patch_width, pf = frame_patch_size)\n",
    "        )\n",
    "        \n",
    "    def get_encoded_fmap_size(self, image_size):\n",
    "        return image_size // self.patch_size\n",
    "\n",
    "    @property\n",
    "    def last_dec_layer(self):\n",
    "        return self.decoder[-2][-1].weight\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d56a8e6-16bf-4e04-b9df-9c8380b2035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #5, 13, 640, 1280\n",
    "\n",
    "# image_height = 640 # 640\n",
    "# patch_height = 32\n",
    "# image_width = 1280 # 1280\n",
    "# patch_width = 16\n",
    "# frames = 16\n",
    "# frame_patch_size = 8\n",
    "\n",
    "# channels = 5\n",
    "# dim = 500\n",
    "# layers = 4\n",
    "# dim_head = 30\n",
    "# mlp_dim = 30\n",
    "# heads = 8\n",
    "# depth = 4\n",
    "\n",
    "# num_embeddings = 500\n",
    "# embedding_dim = 100\n",
    "# commitment_cost = 1.0\n",
    "# decay = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361644e9-2d55-4ba6-b37d-48450d165239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor = torch.randn(1, channels, frames, image_height, image_width).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf6dd7d-fb83-401e-8fc5-e0d4775b1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ViTEncDec(\n",
    "#     image_height, \n",
    "#     patch_height, \n",
    "#     image_width,\n",
    "#     patch_width,\n",
    "#     frames, \n",
    "#     frame_patch_size,\n",
    "#     dim,\n",
    "#     channels,\n",
    "#     depth,\n",
    "#     heads,\n",
    "#     dim_head,\n",
    "#     mlp_dim\n",
    "# ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e47916-f34d-48a8-847c-1685a820328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ca5133-e431-4d86-8ed2-18ff504bb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model.encoder(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16caa8a2-58eb-422d-9950-715ff762ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "026e1272-115b-487e-9f34-eed28faa8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantizer = LFQ(\n",
    "#     codebook_size = 65536,      # codebook size, must be a power of 2\n",
    "#     dim = 500,                   # this is the input feature dimension, defaults to log2(codebook_size) if not defined\n",
    "#     entropy_loss_weight = 0.1,  # how much weight to place on entropy loss\n",
    "#     diversity_gamma = 1.        # within entropy loss, how much weight to give to diversity of codes, taken from https://arxiv.org/abs/1911.05894\n",
    "# ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb547e1-bd4b-4e1a-bab6-37bc297d9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantized, indices, entropy_aux_loss = quantizer(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e13457ce-e2b4-4977-85dd-a974cb4ebc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49285bb4-03b0-498f-b9ae-6e037e5a1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded = model.decoder(quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf43d2f-8a9e-4ec4-8199-588d4625f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b6b2b5b-eec5-41f9-9d39-6ea6b90239e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ViT3DVAE(nn.Module):\n",
    "#     def __init__(self, \n",
    "#                 image_height, \n",
    "#                 patch_height, \n",
    "#                 image_width,\n",
    "#                 patch_width,\n",
    "#                 frames, \n",
    "#                 frame_patch_size,\n",
    "#                 dim,\n",
    "#                 channels,\n",
    "#                 depth,\n",
    "#                 heads,\n",
    "#                 dim_head,\n",
    "#                 mlp_dim,\n",
    "#                 num_embeddings, \n",
    "#                 embedding_dim,     \n",
    "#                 commitment_cost=0.25, \n",
    "#                 entropy_loss_weight=0.1,\n",
    "#                 diversity_gamma=1.0):\n",
    "        \n",
    "#         super(ViT3DVAE, self).__init__()\n",
    "        \n",
    "#         vit = ViTEncDec(\n",
    "#             image_height, \n",
    "#             patch_height, \n",
    "#             image_width,\n",
    "#             patch_width,\n",
    "#             frames, \n",
    "#             frame_patch_size,\n",
    "#             dim,\n",
    "#             channels,\n",
    "#             depth,\n",
    "#             heads,\n",
    "#             dim_head,\n",
    "#             mlp_dim\n",
    "#         )\n",
    "        \n",
    "#         self.encoder = vit.encoder\n",
    "#         self.decoder = vit.decoder\n",
    "#         self._vq_vae = LFQ(\n",
    "#             codebook_size = 65536,      # codebook size, must be a power of 2\n",
    "#             dim = dim,                  # this is the input feature dimension, defaults to log2(codebook_size) if not defined\n",
    "#             entropy_loss_weight = entropy_loss_weight,  # how much weight to place on entropy loss\n",
    "#             diversity_gamma = diversity_gamma        # within entropy loss, how much weight to give to diversity of codes, taken from https://arxiv.org/abs/1911.05894\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         z = self.encoder(x)\n",
    "#         z, indices, commit_loss = self._vq_vae(z)\n",
    "#         z = self.decoder(z)\n",
    "#         return z, indices, commit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d0b929f-12c7-4f65-a45e-7c2077aef399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ViT3DVAE(\n",
    "#     image_height, \n",
    "#     patch_height, \n",
    "#     image_width,\n",
    "#     patch_width,\n",
    "#     frames, \n",
    "#     frame_patch_size,\n",
    "#     dim,\n",
    "#     channels,\n",
    "#     depth,\n",
    "#     heads,\n",
    "#     dim_head,\n",
    "#     mlp_dim,\n",
    "#     num_embeddings,\n",
    "#     embedding_dim, \n",
    "# ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f2d9ffb-d776-418d-95e6-6a25ab070de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fedba1a8-33b0-4654-917a-d1510611e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f7ad0-c525-40a3-8571-111751a51ac6",
   "metadata": {},
   "source": [
    "### Wills batcher: (B, V, L, H, W) -- (B, 5 variables, 13 levels, 640, 1280)\n",
    "\n",
    "U, V, T, specific humidity (Q), and surface pressure \n",
    "There will also be a single static topography variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37cddd10-49eb-4ac1-9155-3353a6db4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims,\n",
    "        channels = 3,\n",
    "        groups = 16,\n",
    "        init_kernel_size = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_pairs = zip(dims[:-1], dims[1:])\n",
    "\n",
    "        self.layers = nn.ModuleList([nn.Sequential(nn.Conv2d(channels, dims[0], init_kernel_size, padding = init_kernel_size // 2), nn.LeakyReLU(0.1))])\n",
    "\n",
    "        for dim_in, dim_out in dim_pairs:\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Conv2d(dim_in, dim_out, 4, stride = 2, padding = 1),\n",
    "                nn.GroupNorm(groups, dim_out),\n",
    "                nn.LeakyReLU(0.1)\n",
    "            ))\n",
    "\n",
    "        dim = dims[-1]\n",
    "        self.to_logits = nn.Sequential( # return 5 x 5, for PatchGAN-esque training\n",
    "            nn.Conv2d(dim, dim, 1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(dim, 1, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for net in self.layers:\n",
    "            x = net(x)\n",
    "\n",
    "        return self.to_logits(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3266f394-c344-4e5f-b78b-06049b824635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_discr_loss(fake, real):\n",
    "    return (F.relu(1 + fake) + F.relu(1 - real)).mean()\n",
    "\n",
    "def hinge_gen_loss(fake):\n",
    "    return -fake.mean()\n",
    "\n",
    "def grad_layer_wrt_loss(loss, layer):\n",
    "    return torch_grad(\n",
    "        outputs = loss,\n",
    "        inputs = layer,\n",
    "        grad_outputs = torch.ones_like(loss),\n",
    "        retain_graph = True\n",
    "    )[0].detach()\n",
    "\n",
    "def safe_div(numer, denom, eps = 1e-8):\n",
    "    return numer / (denom + eps)\n",
    "\n",
    "def bce_discr_loss(fake, real):\n",
    "    return (-log(1 - torch.sigmoid(fake)) - log(torch.sigmoid(real))).mean()\n",
    "\n",
    "def bce_gen_loss(fake):\n",
    "    return -log(torch.sigmoid(fake)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42e8c391-c55b-43f3-a130-8e09a604eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQGanVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_height, \n",
    "        patch_height, \n",
    "        image_width,\n",
    "        patch_width,\n",
    "        frames, \n",
    "        frame_patch_size,\n",
    "        dim,\n",
    "        channels,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        mlp_dim,\n",
    "        l2_recon_loss = False,\n",
    "        use_hinge_loss = True,\n",
    "        vgg = None,\n",
    "        use_vgg_and_gan = True,\n",
    "        vq_codebook_dim = 256,\n",
    "        vq_codebook_size = 65536,\n",
    "        vq_entropy_loss_weight = 0.8,\n",
    "        vq_diversity_gamma = 1.,\n",
    "        discr_layers = 4,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.codebook_size = vq_codebook_size\n",
    "\n",
    "        self.enc_dec = ViTEncDec(\n",
    "            image_height, \n",
    "            patch_height, \n",
    "            image_width,\n",
    "            patch_width,\n",
    "            frames, \n",
    "            frame_patch_size,\n",
    "            dim,\n",
    "            channels,\n",
    "            depth,\n",
    "            heads,\n",
    "            dim_head,\n",
    "            mlp_dim\n",
    "        )\n",
    "\n",
    "        self.vq = self._vq_vae = LFQ(\n",
    "            codebook_size = vq_codebook_size,      # codebook size, must be a power of 2\n",
    "            dim = dim,                  # this is the input feature dimension, defaults to log2(codebook_size) if not defined\n",
    "            entropy_loss_weight = vq_entropy_loss_weight,  # how much weight to place on entropy loss\n",
    "            diversity_gamma = vq_diversity_gamma        # within entropy loss, how much weight to give to diversity of codes, taken from https://arxiv.org/abs/1911.05894\n",
    "        )\n",
    "\n",
    "        # reconstruction loss\n",
    "        self.recon_loss_fn = F.mse_loss if l2_recon_loss else F.l1_loss\n",
    "        self.vgg = torchvision.models.vgg16(pretrained = True)\n",
    "        self.vgg.features[0] = torch.nn.Conv2d(channels, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.vgg.classifier = nn.Sequential(*self.vgg.classifier[:-2])\n",
    "\n",
    "        # gan related losses\n",
    "        layer_mults = list(map(lambda t: 2 ** t, range(discr_layers)))\n",
    "        layer_dims = [dim * mult for mult in layer_mults]\n",
    "        dims = (dim, *layer_dims)\n",
    "\n",
    "        self.discr = Discriminator(dims = dims, channels = channels)\n",
    "        self.discr_loss = hinge_discr_loss if use_hinge_loss else bce_discr_loss\n",
    "        self.gen_loss = hinge_gen_loss if use_hinge_loss else bce_gen_loss\n",
    "        \n",
    "        self.use_vgg_and_gan = use_vgg_and_gan\n",
    "\n",
    "    @property\n",
    "    def encoded_dim(self):\n",
    "        return self.enc_dec.encoded_dim\n",
    "\n",
    "    def get_encoded_fmap_size(self, image_size):\n",
    "        return self.enc_dec.get_encoded_fmap_size(image_size)\n",
    "\n",
    "    def copy_for_eval(self):\n",
    "        device = next(self.parameters()).device\n",
    "        vae_copy = copy.deepcopy(self.cpu())\n",
    "\n",
    "        if vae_copy.use_vgg_and_gan:\n",
    "            del vae_copy.discr\n",
    "            del vae_copy.vgg\n",
    "\n",
    "        vae_copy.eval()\n",
    "        return vae_copy.to(device)\n",
    "\n",
    "    def state_dict(self, *args, **kwargs):\n",
    "        return super().state_dict(*args, **kwargs)\n",
    "\n",
    "    def load_state_dict(self, *args, **kwargs):\n",
    "        return super().load_state_dict(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def codebook(self):\n",
    "        return self.vq.codebook\n",
    "\n",
    "    def encode(self, fmap):\n",
    "        fmap = self.enc_dec.encode(fmap)\n",
    "        return fmap\n",
    "\n",
    "    def decode(self, fmap, return_indices_and_loss = False):\n",
    "        fmap, indices, commit_loss = self.vq(fmap)\n",
    "\n",
    "        fmap = self.enc_dec.decode(fmap)\n",
    "\n",
    "        if not return_indices_and_loss:\n",
    "            return fmap\n",
    "\n",
    "        return fmap, indices, commit_loss\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        img,\n",
    "        return_loss = False,\n",
    "        return_discr_loss = False,\n",
    "        return_recons = False,\n",
    "        add_gradient_penalty = True\n",
    "    ):\n",
    "        batch, channels, frames, height, width, device = *img.shape, img.device\n",
    "        \n",
    "        #assert height == self.image_size and width == self.image_size, 'height and width of input image must be equal to {self.image_size}'\n",
    "        #assert channels == self.channels, 'number of channels on image or sketch is not equal to the channels set on this VQGanVAE'\n",
    "\n",
    "        fmap = self.encode(img)\n",
    "        fmap, indices, commit_loss = self.decode(fmap, return_indices_and_loss = True)\n",
    "\n",
    "        if not return_loss and not return_discr_loss:\n",
    "            return fmap\n",
    "\n",
    "        assert return_loss ^ return_discr_loss, 'you should either return autoencoder loss or discriminator loss, but not both'\n",
    "\n",
    "        # whether to return discriminator loss\n",
    "\n",
    "        if return_discr_loss:\n",
    "            assert exists(self.discr), 'discriminator must exist to train it'\n",
    "\n",
    "            fmap.detach_()\n",
    "            img.requires_grad_()\n",
    "\n",
    "            fmap_discr_logits, img_discr_logits = map(self.discr, (fmap, img))\n",
    "\n",
    "            discr_loss = self.discr_loss(fmap_discr_logits, img_discr_logits)\n",
    "\n",
    "            if add_gradient_penalty:\n",
    "                gp = gradient_penalty(img, img_discr_logits)\n",
    "                loss = discr_loss + gp\n",
    "\n",
    "            if return_recons:\n",
    "                return loss, fmap\n",
    "\n",
    "            return loss\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = self.recon_loss_fn(fmap, img)\n",
    "\n",
    "        # early return if training on grayscale\n",
    "        if not self.use_vgg_and_gan:\n",
    "            if return_recons:\n",
    "                return recon_loss, fmap\n",
    "\n",
    "            return recon_loss\n",
    "\n",
    "        # perceptual loss\n",
    "        img_vgg_input = img\n",
    "        fmap_vgg_input = fmap\n",
    "        \n",
    "        # in the dall-e example, there are no frames (pressure levels). here we loop over them and sum the losses\n",
    "        loss = recon_loss + commit_loss\n",
    "        for i in range(img_vgg_input.shape[2]):\n",
    "            # Get the i-th frame from the original and reconstructed videos\n",
    "            img_frame = img_vgg_input[:, :, i, :, :]\n",
    "            fmap_frame = fmap_vgg_input[:, :, i, :, :]\n",
    "\n",
    "            # Compute VGG features for the original and reconstructed frames\n",
    "            img_vgg_feats = self.vgg(img_frame)\n",
    "            recon_vgg_feats = self.vgg(fmap_frame)\n",
    "\n",
    "            # Compute the perceptual loss (MSE loss between VGG features)\n",
    "            perceptual_loss = F.mse_loss(img_vgg_feats, recon_vgg_feats)\n",
    "\n",
    "            # generator loss\n",
    "            gen_loss = self.gen_loss(self.discr(fmap_frame))\n",
    "\n",
    "            # calculate adaptive weight\n",
    "            last_dec_layer = self.enc_dec.last_dec_layer\n",
    "            norm_grad_wrt_gen_loss = grad_layer_wrt_loss(gen_loss, last_dec_layer).norm(p = 2)\n",
    "            norm_grad_wrt_perceptual_loss = grad_layer_wrt_loss(perceptual_loss, last_dec_layer).norm(p = 2)\n",
    "\n",
    "            #print(norm_grad_wrt_perceptual_loss, norm_grad_wrt_gen_loss)\n",
    "            adaptive_weight = safe_div(norm_grad_wrt_perceptual_loss, norm_grad_wrt_gen_loss)\n",
    "            adaptive_weight.clamp_(max = 1e4)\n",
    "\n",
    "            # combine losses\n",
    "            loss += (perceptual_loss + adaptive_weight * gen_loss)\n",
    "\n",
    "        if return_recons:\n",
    "            return loss, fmap\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0cb6199-a4dc-48ed-818d-0fe4b68aaff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 640 # 640\n",
    "patch_height = 64\n",
    "image_width = 1280 # 1280\n",
    "patch_width = 64\n",
    "frames = 16\n",
    "frame_patch_size = 4\n",
    "\n",
    "channels = 5\n",
    "dim = 128\n",
    "layers = 4\n",
    "dim_head = 30\n",
    "mlp_dim = 30\n",
    "heads = 8\n",
    "depth = 4\n",
    "\n",
    "vq_codebook_dim = 128\n",
    "vq_codebook_size = 2 ** 16\n",
    "vq_entropy_loss_weight = 0.8\n",
    "vq_diversity_gamma = 1.\n",
    "discr_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edf0b1e9-9959-442d-95c6-0fd15b9dbd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(1, channels, frames, image_height, image_width).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "300d1aae-2972-429b-b2ed-a7cb1bfc9b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/schreck/miniconda3/envs/evidential/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/glade/work/schreck/miniconda3/envs/evidential/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = VQGanVAE(\n",
    "    image_height, \n",
    "    patch_height, \n",
    "    image_width,\n",
    "    patch_width,\n",
    "    frames, \n",
    "    frame_patch_size,\n",
    "    dim,\n",
    "    channels,\n",
    "    depth,\n",
    "    heads,\n",
    "    dim_head,\n",
    "    mlp_dim,\n",
    "    vq_codebook_dim=vq_codebook_dim,\n",
    "    vq_codebook_size=vq_codebook_size,\n",
    "    vq_entropy_loss_weight=vq_entropy_loss_weight,\n",
    "    vq_diversity_gamma=vq_diversity_gamma,\n",
    "    discr_layers=discr_layers\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f897b886-5d10-41ee-85df-3cf5df900d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d255493-1b78-4c61-83bb-6883114dd0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "873d95ae-5689-4faf-a291-5b5a766e16f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(input_tensor, return_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f80710a6-a322-4fbc-a72e-71d5aa486a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3894, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcf54c7e-6876-4fb2-b456-aa08ba27ea9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 200171329\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters in the model: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704d247-fec7-4ee6-8d49-e52b25a110f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-evidential]",
   "language": "python",
   "name": "conda-env-miniconda3-evidential-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
